{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "DataSet = \"model_input.npy\"  # 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"获取数据\"\"\"\n",
    "    arr=np.load(filename,allow_pickle=True)\n",
    "    arr1=[]\n",
    "    arr2=[]\n",
    "    for i in range(len(arr)):\n",
    "        arr1.append(arr[i][0])\n",
    "        arr2.append(arr[i][1])\n",
    "    return arr1,arr2\n",
    "    \n",
    "class CifarData:\n",
    "    \"\"\"打乱数据集\"\"\"\n",
    "    def __init__(self, filenames, need_shuffle):  # 训练集需要打乱\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        '''for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)'''\n",
    "        data,labels=load_data(filenames)\n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        self._data = np.vstack(all_data)  # 转为纵向矩阵,10000组，每组3072数\n",
    "        self._data = np.reshape(self._data,(len(self._data),-1))  # 转为纵向矩阵,10000组，每组3072数\n",
    "        # print(self._data)\n",
    "        self._labels = np.hstack(all_labels)  # 转为横向矩阵，10000个数\n",
    "        # print(self._data.shape)\n",
    "        # print(self._labels.shape)\n",
    "        \n",
    "        self._num_examples = self._data.shape[0]  # 训练集总数量\n",
    "        # print(self._num_examples)\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0  # 当前遍历数据集的位置\n",
    "        if self._need_shuffle:  # 判断是否需要打乱数据\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):  # 打乱数据\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):  # 数据分组，每次取不同的组\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:  # 考察位置大于总数，重新打乱数据，重新分组\n",
    "            if self._need_shuffle:  # 可以打乱\n",
    "                self._shuffle_data()  # 重新打乱\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:  # 分块大小过大\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "train_data=CifarData(\"train_file_1.npy\",True)\n",
    "test_data=CifarData(\"test_file_1.npy\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_wrapper(inputs,name,output_channel=32,\\\n",
    "    kernel_size=(3,3),strides=1,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n",
    "    \"\"\"卷积层装饰器\"\"\"\n",
    "    # with batch normalization:conv->bn->activation\n",
    "    with tf.name_scope(name):\n",
    "        conv2d=tf.layers.conv2d(inputs,\n",
    "                         output_channel,\n",
    "                         kernel_size,\n",
    "                         strides=strides,\n",
    "                         padding='same',\n",
    "                         activation=None,\n",
    "                         kernel_initializer=kernel_initializer,\n",
    "                         data_format='channels_first',\n",
    "                         name=name+'/conv2d')\n",
    "        bn=tf.layers.batch_normalization(conv2d,training=True)\n",
    "        return activation(bn)\n",
    "        \n",
    "def pooling_wrapper(inputs,name):\n",
    "    \"\"\"池化层装饰器\"\"\"\n",
    "    return tf.layers.max_pooling2d(inputs,(2,2),(2,2),name=name,padding='same',data_format='channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算图\n",
    "x = tf.placeholder(tf.float32, [None, 11264])  # 设置占位符\n",
    "x_image=tf.reshape(x,[-1,1,88,128])  # 图像规模\n",
    "x_image=tf.transpose(x_image,perm=[0,2,3,1])  # 通道转换\n",
    "y = tf.placeholder(tf.int64, [None])  # y为的标注\n",
    "\n",
    "#conv1\n",
    "conv1_1=conv_wrapper(x_image,'conv1_1')\n",
    "conv1_2=conv_wrapper(conv1_1,'conv1_2')\n",
    "conv1_3=conv_wrapper(conv1_2,'conv1_3')\n",
    "pooling1=pooling_wrapper(conv1_3,'pool1')\n",
    "#conv2\n",
    "conv2_1=conv_wrapper(pooling1,'conv2_1',output_channel=64)\n",
    "conv2_2=conv_wrapper(conv2_1,'conv2_2',output_channel=64)\n",
    "conv2_3=conv_wrapper(conv2_2,'conv2_3',output_channel=64)\n",
    "pooling2=pooling_wrapper(conv2_3,'pool2')\n",
    "#conv3\n",
    "conv3_1=conv_wrapper(pooling2,'conv3_1',output_channel=64)\n",
    "conv3_2=conv_wrapper(conv3_1,'conv3_2',output_channel=64)\n",
    "conv3_3=conv_wrapper(conv3_2,'conv3_3',output_channel=64)\n",
    "pooling3=pooling_wrapper(conv3_3,'pool3')\n",
    "\n",
    "#flat(平坦化)\n",
    "flatten=tf.layers.flatten(pooling3)  # 在保留第0轴的情况下对输入的张量进行Flatten(扁平化)\n",
    "#输出 全连接层 输出形状[?,10]\n",
    "logits=tf.layers.dense(flatten,10)  # 全连接层\n",
    "\n",
    "loss=tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits)  # 交叉熵损失函数：y_->softmax,y->onhot,loss=ylogy_\n",
    "\n",
    "predict = tf.argmax(logits,1)  # 样本中分布的最大值的位置，得到index\n",
    "correct_prediction = tf.equal(predict, y)  # 相等为1，不相等为0\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))  # 平均数即为准确率\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)  # adam优化器，定义梯度下降方法，使损失函数最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Step: 100, loss: 0.35810, acc: 0.90000\n",
      "Train-Step: 200, loss: 0.40720, acc: 0.87000\n",
      "Train-Step: 300, loss: 0.27852, acc: 0.91000\n",
      "Train-Step: 400, loss: 0.23203, acc: 0.93000\n",
      "Train-Step: 500, loss: 0.25536, acc: 0.90000\n",
      "Train-Step: 600, loss: 0.48447, acc: 0.85000\n",
      "Train-Step: 700, loss: 0.22241, acc: 0.91000\n",
      "Train-Step: 800, loss: 0.52659, acc: 0.88000\n",
      "Train-Step: 900, loss: 0.28135, acc: 0.92000\n",
      "Train-Step: 1000, loss: 0.21112, acc: 0.93000\n",
      "Train-Step: 1100, loss: 0.21257, acc: 0.93000\n",
      "Train-Step: 1200, loss: 0.33927, acc: 0.93000\n",
      "Train-Step: 1300, loss: 0.34803, acc: 0.87000\n",
      "Train-Step: 1400, loss: 0.24197, acc: 0.92000\n",
      "Train-Step: 1500, loss: 0.22259, acc: 0.92000\n",
      "Train-Step: 1600, loss: 0.14843, acc: 0.95000\n",
      "Train-Step: 1700, loss: 0.21237, acc: 0.91000\n",
      "Train-Step: 1800, loss: 0.10804, acc: 0.96000\n",
      "Train-Step: 1900, loss: 0.13491, acc: 0.95000\n",
      "Train-Step: 2000, loss: 0.14829, acc: 0.96000\n",
      "Test: 2000, acc: 0.97500\n",
      "Train-Step: 2100, loss: 0.10564, acc: 0.96000\n",
      "Train-Step: 2200, loss: 0.16802, acc: 0.93000\n",
      "Train-Step: 2300, loss: 0.25142, acc: 0.88000\n",
      "Train-Step: 2400, loss: 0.14210, acc: 0.96000\n",
      "Train-Step: 2500, loss: 0.14858, acc: 0.93000\n",
      "Train-Step: 2600, loss: 0.14086, acc: 0.95000\n",
      "Train-Step: 2700, loss: 0.16357, acc: 0.96000\n",
      "Train-Step: 2800, loss: 0.10357, acc: 0.97000\n",
      "Train-Step: 2900, loss: 0.11649, acc: 0.95000\n",
      "Train-Step: 3000, loss: 0.26445, acc: 0.89000\n",
      "Train-Step: 3100, loss: 0.05305, acc: 0.99000\n",
      "Train-Step: 3200, loss: 0.17729, acc: 0.92000\n",
      "Train-Step: 3300, loss: 0.12712, acc: 0.95000\n",
      "Train-Step: 3400, loss: 0.17046, acc: 0.94000\n",
      "Train-Step: 3500, loss: 0.45675, acc: 0.85000\n",
      "Train-Step: 3600, loss: 0.11639, acc: 0.96000\n",
      "Train-Step: 3700, loss: 0.19388, acc: 0.92000\n",
      "Train-Step: 3800, loss: 0.17509, acc: 0.94000\n",
      "Train-Step: 3900, loss: 0.24467, acc: 0.90000\n",
      "Train-Step: 4000, loss: 0.11333, acc: 0.96000\n",
      "Test: 4000, acc: 0.97500\n",
      "Train-Step: 4100, loss: 0.10823, acc: 0.97000\n",
      "Train-Step: 4200, loss: 0.24626, acc: 0.91000\n",
      "Train-Step: 4300, loss: 0.16231, acc: 0.94000\n",
      "Train-Step: 4400, loss: 0.08908, acc: 0.97000\n",
      "Train-Step: 4500, loss: 0.15067, acc: 0.97000\n",
      "Train-Step: 4600, loss: 0.17375, acc: 0.93000\n",
      "Train-Step: 4700, loss: 0.13981, acc: 0.96000\n",
      "Train-Step: 4800, loss: 0.20720, acc: 0.93000\n",
      "Train-Step: 4900, loss: 0.14071, acc: 0.93000\n",
      "Train-Step: 5000, loss: 0.22679, acc: 0.92000\n",
      "Train-Step: 5100, loss: 0.15082, acc: 0.95000\n",
      "Train-Step: 5200, loss: 0.56895, acc: 0.85000\n",
      "Train-Step: 5300, loss: 0.38463, acc: 0.90000\n",
      "Train-Step: 5400, loss: 0.44332, acc: 0.88000\n",
      "Train-Step: 5500, loss: 0.58378, acc: 0.83000\n",
      "Train-Step: 5600, loss: 0.41588, acc: 0.87000\n",
      "Train-Step: 5700, loss: 0.55357, acc: 0.82000\n",
      "Train-Step: 5800, loss: 0.46265, acc: 0.86000\n",
      "Train-Step: 5900, loss: 0.47571, acc: 0.85000\n",
      "Train-Step: 6000, loss: 0.49874, acc: 0.85000\n",
      "Test: 6000, acc: 0.96000\n",
      "Train-Step: 6100, loss: 0.61200, acc: 0.84000\n",
      "Train-Step: 6200, loss: 0.52819, acc: 0.84000\n",
      "Train-Step: 6300, loss: 0.61657, acc: 0.79000\n",
      "Train-Step: 6400, loss: 0.38762, acc: 0.89000\n",
      "Train-Step: 6500, loss: 0.50130, acc: 0.85000\n",
      "Train-Step: 6600, loss: 0.63089, acc: 0.81000\n",
      "Train-Step: 6700, loss: 0.44800, acc: 0.88000\n",
      "Train-Step: 6800, loss: 0.42782, acc: 0.89000\n",
      "Train-Step: 6900, loss: 0.56228, acc: 0.82000\n",
      "Train-Step: 7000, loss: 0.52363, acc: 0.83000\n",
      "Train-Step: 7100, loss: 0.53487, acc: 0.87000\n",
      "Train-Step: 7200, loss: 0.68515, acc: 0.79000\n",
      "Train-Step: 7300, loss: 0.35407, acc: 0.90000\n",
      "Train-Step: 7400, loss: 0.42270, acc: 0.90000\n",
      "Train-Step: 7500, loss: 0.59577, acc: 0.79000\n",
      "Train-Step: 7600, loss: 0.59886, acc: 0.82000\n",
      "Train-Step: 7700, loss: 0.68528, acc: 0.79000\n",
      "Train-Step: 7800, loss: 0.53386, acc: 0.80000\n",
      "Train-Step: 7900, loss: 0.62496, acc: 0.81000\n",
      "Train-Step: 8000, loss: 0.56574, acc: 0.84000\n",
      "Test: 8000, acc: 0.96000\n",
      "Train-Step: 8100, loss: 0.46428, acc: 0.86000\n",
      "Train-Step: 8200, loss: 0.41617, acc: 0.86000\n",
      "Train-Step: 8300, loss: 0.68191, acc: 0.78000\n",
      "Train-Step: 8400, loss: 0.61013, acc: 0.82000\n",
      "Train-Step: 8500, loss: 0.68825, acc: 0.81000\n",
      "Train-Step: 8600, loss: 0.48129, acc: 0.86000\n",
      "Train-Step: 8700, loss: 0.71167, acc: 0.80000\n",
      "Train-Step: 8800, loss: 0.48398, acc: 0.86000\n",
      "Train-Step: 8900, loss: 0.52803, acc: 0.84000\n",
      "Train-Step: 9000, loss: 0.47304, acc: 0.87000\n",
      "Train-Step: 9100, loss: 0.38756, acc: 0.89000\n",
      "Train-Step: 9200, loss: 0.35609, acc: 0.89000\n",
      "Train-Step: 9300, loss: 0.43397, acc: 0.88000\n",
      "Train-Step: 9400, loss: 0.54664, acc: 0.85000\n",
      "Train-Step: 9500, loss: 0.38148, acc: 0.87000\n",
      "Train-Step: 9600, loss: 0.55416, acc: 0.82000\n",
      "Train-Step: 9700, loss: 0.45661, acc: 0.88000\n",
      "Train-Step: 9800, loss: 0.34837, acc: 0.90000\n",
      "Train-Step: 9900, loss: 0.27217, acc: 0.90000\n",
      "Train-Step: 10000, loss: 0.25786, acc: 0.91000\n",
      "Test: 10000, acc: 0.51500\n",
      "Train-Step: 10100, loss: 0.36507, acc: 0.88000\n",
      "Train-Step: 10200, loss: 0.34860, acc: 0.89000\n",
      "Train-Step: 10300, loss: 0.21344, acc: 0.90000\n",
      "Train-Step: 10400, loss: 0.22756, acc: 0.89000\n",
      "Train-Step: 10500, loss: 0.28785, acc: 0.88000\n",
      "Train-Step: 10600, loss: 0.28618, acc: 0.90000\n",
      "Train-Step: 10700, loss: 0.25839, acc: 0.89000\n",
      "Train-Step: 10800, loss: 0.35248, acc: 0.87000\n",
      "Train-Step: 10900, loss: 0.43361, acc: 0.78000\n",
      "Train-Step: 11000, loss: 0.41687, acc: 0.85000\n",
      "Train-Step: 11100, loss: 0.32170, acc: 0.89000\n",
      "Train-Step: 11200, loss: 0.43000, acc: 0.83000\n",
      "Train-Step: 11300, loss: 0.25967, acc: 0.88000\n",
      "Train-Step: 11400, loss: 0.21768, acc: 0.92000\n",
      "Train-Step: 11500, loss: 0.35889, acc: 0.83000\n",
      "Train-Step: 11600, loss: 0.29184, acc: 0.92000\n",
      "Train-Step: 11700, loss: 0.29155, acc: 0.92000\n",
      "Train-Step: 11800, loss: 0.23628, acc: 0.90000\n",
      "Train-Step: 11900, loss: 0.29679, acc: 0.89000\n",
      "Train-Step: 12000, loss: 0.37491, acc: 0.87000\n",
      "Test: 12000, acc: 0.51500\n",
      "Train-Step: 12100, loss: 0.36048, acc: 0.86000\n",
      "Train-Step: 12200, loss: 0.23802, acc: 0.92000\n",
      "Train-Step: 12300, loss: 0.35413, acc: 0.91000\n",
      "Train-Step: 12400, loss: 0.27753, acc: 0.89000\n",
      "Train-Step: 12500, loss: 0.31702, acc: 0.88000\n",
      "Train-Step: 12600, loss: 0.34911, acc: 0.90000\n",
      "Train-Step: 12700, loss: 0.37143, acc: 0.89000\n",
      "Train-Step: 12800, loss: 0.30697, acc: 0.86000\n",
      "Train-Step: 12900, loss: 0.36788, acc: 0.85000\n",
      "Train-Step: 13000, loss: 0.30610, acc: 0.89000\n",
      "Train-Step: 13100, loss: 0.24242, acc: 0.89000\n",
      "Train-Step: 13200, loss: 0.28121, acc: 0.92000\n",
      "Train-Step: 13300, loss: 0.37311, acc: 0.91000\n",
      "Train-Step: 13400, loss: 0.36460, acc: 0.83000\n",
      "Train-Step: 13500, loss: 0.32986, acc: 0.89000\n",
      "Train-Step: 13600, loss: 0.25706, acc: 0.89000\n",
      "Train-Step: 13700, loss: 0.30996, acc: 0.91000\n",
      "Train-Step: 13800, loss: 0.24963, acc: 0.92000\n",
      "Train-Step: 13900, loss: 0.29071, acc: 0.90000\n",
      "Train-Step: 14000, loss: 0.34985, acc: 0.88000\n",
      "Test: 14000, acc: 0.89500\n",
      "Train-Step: 14100, loss: 0.31338, acc: 0.90000\n",
      "Train-Step: 14200, loss: 0.16142, acc: 0.93000\n",
      "Train-Step: 14300, loss: 0.30629, acc: 0.91000\n",
      "Train-Step: 14400, loss: 0.30780, acc: 0.87000\n",
      "Train-Step: 14500, loss: 0.18288, acc: 0.91000\n",
      "Train-Step: 14600, loss: 0.42346, acc: 0.85000\n",
      "Train-Step: 14700, loss: 0.26435, acc: 0.89000\n",
      "Train-Step: 14800, loss: 0.22329, acc: 0.93000\n",
      "Train-Step: 14900, loss: 0.32437, acc: 0.87000\n",
      "Train-Step: 15000, loss: 0.32688, acc: 0.87000\n",
      "Train-Step: 15100, loss: 0.18473, acc: 0.91000\n",
      "Train-Step: 15200, loss: 0.17426, acc: 0.93000\n",
      "Train-Step: 15300, loss: 0.22932, acc: 0.93000\n",
      "Train-Step: 15400, loss: 0.21937, acc: 0.94000\n",
      "Train-Step: 15500, loss: 0.35113, acc: 0.89000\n",
      "Train-Step: 15600, loss: 0.29122, acc: 0.89000\n",
      "Train-Step: 15700, loss: 0.33491, acc: 0.88000\n",
      "Train-Step: 15800, loss: 0.36642, acc: 0.86000\n",
      "Train-Step: 15900, loss: 0.31423, acc: 0.90000\n",
      "Train-Step: 16000, loss: 0.28721, acc: 0.89000\n",
      "Test: 16000, acc: 0.51500\n",
      "Train-Step: 16100, loss: 0.26978, acc: 0.88000\n",
      "Train-Step: 16200, loss: 0.39698, acc: 0.85000\n",
      "Train-Step: 16300, loss: 0.22125, acc: 0.92000\n",
      "Train-Step: 16400, loss: 0.47204, acc: 0.80000\n",
      "Train-Step: 16500, loss: 0.31560, acc: 0.88000\n",
      "Train-Step: 16600, loss: 0.28260, acc: 0.89000\n",
      "Train-Step: 16700, loss: 0.33171, acc: 0.87000\n",
      "Train-Step: 16800, loss: 0.20236, acc: 0.88000\n",
      "Train-Step: 16900, loss: 0.34544, acc: 0.88000\n",
      "Train-Step: 17000, loss: 0.27520, acc: 0.90000\n",
      "Train-Step: 17100, loss: 0.27939, acc: 0.91000\n",
      "Train-Step: 17200, loss: 0.32127, acc: 0.88000\n",
      "Train-Step: 17300, loss: 0.26886, acc: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Step: 17400, loss: 0.33456, acc: 0.87000\n",
      "Train-Step: 17500, loss: 0.32392, acc: 0.86000\n",
      "Train-Step: 17600, loss: 0.40147, acc: 0.87000\n",
      "Train-Step: 17700, loss: 0.43120, acc: 0.84000\n",
      "Train-Step: 17800, loss: 0.40971, acc: 0.83000\n",
      "Train-Step: 17900, loss: 0.23102, acc: 0.92000\n",
      "Train-Step: 18000, loss: 0.30740, acc: 0.91000\n",
      "Test: 18000, acc: 0.51500\n",
      "Train-Step: 18100, loss: 0.50038, acc: 0.81000\n",
      "Train-Step: 18200, loss: 0.30162, acc: 0.91000\n",
      "Train-Step: 18300, loss: 0.40962, acc: 0.87000\n",
      "Train-Step: 18400, loss: 0.27289, acc: 0.88000\n",
      "Train-Step: 18500, loss: 0.23104, acc: 0.93000\n",
      "Train-Step: 18600, loss: 0.32493, acc: 0.89000\n",
      "Train-Step: 18700, loss: 0.28293, acc: 0.92000\n",
      "Train-Step: 18800, loss: 0.33070, acc: 0.87000\n",
      "Train-Step: 18900, loss: 0.27750, acc: 0.93000\n",
      "Train-Step: 19000, loss: 0.28272, acc: 0.88000\n",
      "Train-Step: 19100, loss: 0.34103, acc: 0.85000\n",
      "Train-Step: 19200, loss: 0.42033, acc: 0.86000\n",
      "Train-Step: 19300, loss: 0.29224, acc: 0.89000\n",
      "Train-Step: 19400, loss: 0.22730, acc: 0.88000\n",
      "Train-Step: 19500, loss: 0.27251, acc: 0.88000\n",
      "Train-Step: 19600, loss: 0.77446, acc: 0.66000\n",
      "Train-Step: 19700, loss: 0.35760, acc: 0.87000\n",
      "Train-Step: 19800, loss: 0.24911, acc: 0.90000\n",
      "Train-Step: 19900, loss: 0.21326, acc: 0.89000\n",
      "Train-Step: 20000, loss: 0.37400, acc: 0.86000\n",
      "Test: 20000, acc: 0.51500\n"
     ]
    }
   ],
   "source": [
    "# 执行计算图\n",
    "init = tf.global_variables_initializer()  # 变量初始化\n",
    "batch_size = 100  # 每次选取数据量\n",
    "train_steps = 20000  # 迭代次数\n",
    "test_steps = 2\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)  # 获取部分数据\n",
    "        # print(batch_labels)\n",
    "        # print(batch_data)\n",
    "        loss_val, acc_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op],\n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels})  # 得到训练的损失值和准确率。对所有的参数计算梯度，然后用梯度去更新参数。\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Train-Step: %d, loss: %4.5f, acc: %4.5f' % (i+1, loss_val, acc_val))\n",
    "        if (i+1) % 2000 == 0:  # 测试集验证\n",
    "            test_data = CifarData(\"test_file_1.npy\", False)  # 每次都重用测试集进行测试\n",
    "            all_test_acc_val = []\n",
    "            for j in range(test_steps): \n",
    "                test_batch_data, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run(\n",
    "                    [accuracy],\n",
    "                    feed_dict = {\n",
    "                        x: test_batch_data, \n",
    "                        y: test_batch_labels\n",
    "                    })  # 模型检测测试集准确率\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('Test: %d, acc: %4.5f' % (i+1, test_acc))  # 计算测试集准确率\n",
    "            saver.save(sess, \"./VGGmodel/VGG\"+str(i)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
